<팀 커널 브리핑>
Mask-RCNN with submission
 - https://www.kaggle.com/hmendonca/mask-rcnn-with-submission
 - 멤버 : 서기원
 - 내용 :


1. 커널 소개
 (1) 커널 소개
  - 이 커널에서는 데이터 세트를 분석하기 위한 기본 사항, 객체 감지 및 인스턴스 분할을 위해 Mask-RCNN 알고리즘에 기반한
디텍터를 사용하여 교육한다.
  - 다만 이 커널의 Mask-RCNN 디텍터 구성은 교육용이기 때문에 최적화 되어 있지는 않다.
import os 
import sys
import random
import math
import numpy as np
import cv2
import matplotlib.pyplot as plt
import json
import pydicom
from imgaug import augmenters as iaa
from tqdm import tqdm
import pandas as pd 
import glob

 (2) 학습 소개
  - 텐서플로우, 케라스, Github 주석을 이용한 흉부 X선 분류
  - 폐 X-Rays Segmantic Segmentation (폐 사진 중 유의미한 부분에 네모박스 하기)
  - Github 주석을 이용한 RSNA Pneumonia detection


2. 소스코드
 (1) 데이터 다운로드
  - Kaggle API를 설치하여 경기 데이터를 다운로드한다.
DATA_DIR = '/kaggle/input' #kaggle/input의 데이터 셋을 이용한다.
ROOT_DIR = '/kaggle/working' #학습된 모델을 kaggle/working 폴더에 저장한다.

 (2) Mask-RCNN 모델 설치
  - Github에서 Matterport의 Mask-RCNN 모델을 설치한다.
!git clone https://www.github.com/matterport/Mask_RCNN.git #git clone을 이용하여 github의 Matterport의 Mask-RCNN 모델을 설치한다.
os.chdir('Mask_RCNN') #chdir을 이용하여 현재의 working 폴더를 Mask_RCNN으로 바꾼다.
#!python setup.py -q install

# Mask RCNN 모델 설치하기
sys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))  # sys.path로 파이썬이 Mask_RCNN의 모듈을 찾을 수 있게 도와준다.
from mrcnn.config import Config
from mrcnn import utils
import mrcnn.model as modellib
from mrcnn import visualize
from mrcnn.model import log

#학습할 수 있는 train 데이터 셋과, 학습한 걸 토대로 검사해볼 수 있는 test 데이터 셋을 변수에 지정한다.
train_dicom_dir = os.path.join(DATA_DIR, 'stage_1_train_images') #os.path로 파일의 경로를 설정하고 기능을 구현할 수 있다.
test_dicom_dir = os.path.join(DATA_DIR, 'stage_1_test_images')

  - Mask-RCNN의 기능 및 클래스
    dicom_fps : dicom 이미지 경로 및 파일의 이름
    image_annotations : annotations는 이미지 중 Detection 하고 싶은 부분의 윤곽을 따 이미지의 구역을 나누는 것이다.
    dataset을 파싱하면 이미지의 파일 이름들과 주석으로된 설명 목록이 반환된다.
def get_dicom_fps(dicom_dir): #함수의 인자로 dicom 폴더 이름을 넣어서 dicom 이미지 파일의 경로 및 파일의 이름을 얻는다.
    dicom_fps = glob.glob(dicom_dir+'/'+'*.dcm')
    return list(set(dicom_fps))

def parse_dataset(dicom_dir, anns): #데이터 셋 파싱하기. 파싱이란 데이터의 정보를 얻고 가공하는 것을 말한다.
    image_fps = get_dicom_fps(dicom_dir) #앞에서 설정한 get_dicom_fps 함수를 이용해서 dicom 이미지 파일의 경로를 image_fps에 저장한다.
    image_annotations = {fp: [] for fp in image_fps} #이미지 중 Detection 하고 싶은 부분의 윤곽을 따 이미지의 구역을 나눈다.
    for index, row in anns.iterrows(): #인덱스의 row에서 iterrows로 각 행의 반환값을 반환한다.
        fp = os.path.join(dicom_dir, row['patientId']+'.dcm') #dicom 이미지에 .dcm과 환자 id 값을 추가한 정보를 fp에 넣고
        image_annotations[fp].append(row) #그걸 이미지에서 윤곽을 딴 한 구역에 추가한다.
    return image_fps, image_annotations #이미지 이름과 이미지 annotations를 반환한다.

# 아래에 사용되는 매개변수 값들은 '시연용'으로 시간을 줄이기 위한 값들이다.
# 따라서 다음의 값들은 최적의 값이 아니다.

class DetectorConfig(Config):
    """RSNA 폐렴 데이터 셋이 폐렴을 탐지(detection)할 수 있게 훈련시키기 위한 구성을 한다.
       아래에서 기본적으로 구성 되어 있는 'DetectorConfig' 클래스의 값을 재정의 한다.
    """
    
    # Config의 이름으로 '사용자가 이해하기 쉬운' 이름을 지정한다.
    NAME = 'pneumonia'

    # 1개의 GPU에 학습을 진행한다. 1개의 GPU당 8개의 이미지를 교육한다. 각 이미지에는 여러 개의 이미지를 넣을 수 있다.
    # 이미지가 작기 때문에, 1개의 GPU에 8개의 이미지를 교육해도 된다.
    GPU_COUNT = 1
    IMAGES_PER_GPU = 8
    
    # resnet은 50을 이용한다.
    BACKBONE = 'resnet50'
    
    # 배경 + 1개의 pneumonia 클래스
    NUM_CLASSES = 2
    
    # 이용할 이미지의 해상도
    IMAGE_MIN_DIM = 256
    IMAGE_MAX_DIM = 256

    # Mask-RCNN에서는 RPN을 내부 네트워크에서 돌리므로 속도가 빨라진다.
    RPN_ANCHOR_SCALES = (32, 64, 128, 256)
    # ROI에 대한 세팅.
    TRAIN_ROIS_PER_IMAGE = 32
    # INSTANCE(데이터 셋에 들어 있는 모든 이미지들)에 대한 설정을 한다.
    MAX_GT_INSTANCES = 3
    DETECTION_MAX_INSTANCES = 3
    # Detection을 해 봤을 때, 폐렴일 확률이 70% 이상이면 '1'로 진단한다. 
    DETECTION_MIN_CONFIDENCE = 0.7
    # Detection을 해 봤을 때, 임계값을 0.1로 설정한다.
    DETECTION_NMS_THRESHOLD = 0.1

    """ 간단하게 알아보는, 이 모델이 돌아가는 원리.
	Learning Rate라는 크기만큼 Step의 수 만큼 깎아 내려가며 최적화된 값을 찾는다.
	이러한 과정을 Epoch 번 만큼 반복한다.
	따라서 일반적으로는, Learning Rate가 적고, Step과 Epoch가 많을 만큼 최적화된 값을 찾기에 좋지만
	Over fitting이 발생할 수 있으므로 주의 할 것!
    """

    # Epoch 한 번 당 Learning Rate의 크기로 200번만큼 깎아 내려 가며 최적화된 값을 찾는다.
    STEPS_PER_EPOCH = 200

# config라는, 폐렴을 탐지(detection)할 수 있게 훈련시키기 위한 구성을 마치고 config를 실행한다.
config = DetectorConfig()
config.display()

# Dataset 클래스로 폐렴을 탐지하는 방법을 교육시킨다.
class DetectorDataset(utils.Dataset):
    """RSNA 폐렴 데이터 셋을 이용해서 폐렴을 탐지 하는 방법을 교육시킨다.
    """

    # __init__은 객체를 생성할 때 자동으로 호출하는 메소드이다.
    # 이미지, 이미지 annotations, 원본 높이, 원본 넓이를 불러 온다.
    def __init__(self, image_fps, image_annotations, orig_height, orig_width):
        super().__init__(self)
        
        # Class를 추가한다.
        self.add_class('pneumonia', 1, 'Lung Opacity')
        
        # Images를 추가한다.
        for i, fp in enumerate(image_fps):
            annotations = image_annotations[fp] # 이미지 annotations
            self.add_image('pneumonia', image_id=i, path=fp, # 이미지_id, annotations, 원본 높이, 원본 넓이
                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)
            
    # 이미지 경로 함수
    def image_reference(self, image_id):
        info = self.image_info[image_id] # ID에 대한 이미지의 정보를 info에 담는다.
        return info['path'] # info의 path를 반환한다.

    # 이미지 로드 함수
    def load_image(self, image_id):
        info = self.image_info[image_id] # ID에 대한 이미지의 정보를 info에 담는다.
        fp = info['path'] # info의 path를 fp에 저장한다.
        ds = pydicom.read_file(fp) # fp의 파일을 읽어서 ds에 저장한다.
        image = ds.pixel_array # ds의 픽셀 배열을 image에 저장한다.
        # 만약 흑백 이미지라면, RGB로 변형해서 일관성을 유지한다.
        if len(image.shape) != 3 or image.shape[2] != 3:
            image = np.stack((image,) * 3, -1)
        return image

    # mask 로드 함수
    def load_mask(self, image_id):
        info = self.image_info[image_id] # ID에 대한 이미지의 정보를 info에 담는다.
        annotations = info['annotations'] # info에 대한 annotations 정보를 annotations에 담는다.
        count = len(annotations) # annotations의 길이를 카운트 한다.
        if count == 0:
            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)
            class_ids = np.zeros((1,), dtype=np.int32)
        else:
            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)
            class_ids = np.zeros((count,), dtype=np.int32)
            for i, a in enumerate(annotations):
                if a['Target'] == 1:
                    x = int(a['x'])
                    y = int(a['y'])
                    w = int(a['width'])
                    h = int(a['height'])
                    mask_instance = mask[:, :, i].copy()
                    cv2.rectangle(mask_instance, (x, y), (x+w, y+h), 255, -1)
                    mask[:, :, i] = mask_instance
                    class_ids[i] = 1
        return mask.astype(np.bool), class_ids.astype(np.int32)

 (3) 주석 데이터를 검사하고, 데이터 셋을 파싱하고, dicom 부분 보기

 (4) 데이터를 training 데이터 셋과 validation 데이터 셋으로 분할하기
  - training set : 데이터 셋으로 학습하여 자동으로 W, b 값을 결정하는 데이터 셋다.
    validation set : 사람의 직관과 경험이 필요한 부분을 조정한 데이터 셋이다. (몇 epoch마다 validation을 시행할지 설정할 수 있다.)
    test set : 학습한 적 없는 새로운 데이터 셋이다.

 (5) DetectorDataset 클래스를 사용하여 training data set을 만들고 준비한다.

 (6) 샘플 주석을 살펴보자. 우리는 좌측 상단의 x,y축과 폭과 높이가 적혀 있는 bounding box를 볼 수 있다.

 (7) bounding box가 있는 임의의 이미지를 살펴본다.

 (8) 이미지 확장하기. 일부 변수를 사용하여 사용자 지정 값으로 이미지를 수정해 본다.

 (9) 모델 학습시키기
  - 모델은 Mask_RCNN 모델이다.
  - basic한 모델을 학습시키는 데도 몇 시간이 걸릴 수 있다는 것을 알아야 한다.
    (하지만 이 모델은 시연용이기 때문에 실행시간을 줄이기 위한 값을 설정했다.)
  - dataset_train과 dataset_val은 DetectorDataset에서 파생되었다.
    DetectorDataset은 annotation data에 있는 이미지 파일 이름과 masks으로 부터 이미지를 로드한다.

 (10) validataion dataset 사용하기
  - validation data set을 이용하여 예상한 박스와 예상 값을 비교해 보자.
  - 다만, 우리는 오직 하나의 epoch만 사용했다는 것을 기억하자.
    너가 코드를 수정해서 더 많은 epoch를 사용한다면 더 좋은 결과를 얻을 수 있을 것이다.

 (11) 최종 단계 - 제출 파일 작성